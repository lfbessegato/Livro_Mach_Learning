{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c1b2708",
   "metadata": {},
   "source": [
    "## Capítulo 9 - Classes Desbalanceadas\n",
    "\n",
    "Se estiver classificando dados e as classes não estiverem relativamente balanceadas quanto ao tamanho, a distorção em direção às classes mais populares poderão transparecer em seu modelo. Por exemplo, se tiver 1 caso positivo e 99 casos negativos, poderá obter 99% de exatidão simplesmente classificando tudo como negativo. Há várias opções para lidar com 'classes desbalanceadas' (imbalanced classes)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c72eef",
   "metadata": {},
   "source": [
    "### Use uma métrica diferente\n",
    "\n",
    "Uma dica é usar uma medida que não seja a exatidão (accuracy) para calibrar os modelos (o AUC é uma boa opção). Precisão (precision) e recall também são ótimas opções quando os tamanhos dos alvos (targets) forem diferentes. No entanto, há outras opções a serem consideradas também."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1723d214",
   "metadata": {},
   "source": [
    "### Algoritmos baseados em árvores e ensembles\n",
    "\n",
    "Modelos baseados em árvore poderão ter um melhor desempenho conforme a distribuição da classe menor. Se os dados tiverem a tendência de estar agrupados, poderão ser mais facilmente classificados. \n",
    "\n",
    "Os métodos de ensemble podem ainda ajudar a extrair as classes minoritárias, Bagging e boosting são opções encontradas em modelos de árvore como florestas aleatórias (random forests) e o XGBoost (Gradient Boosting)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db77354",
   "metadata": {},
   "source": [
    "### Modelos de penalização\n",
    "\n",
    "Mutos modelos de classificação do scikit-learn aceitam o parâmetro 'class_weight'. Defini-lo com 'balanced' tentará regularizar as classes minoritárias e incentivará o modelo a classificá-las corretamente.\n",
    "\n",
    "Como alternativa, pode fazer uma busca em grade (grid search) e especificar as opções de peso, passando um dicionário que mapeie classes e pesos (dando pesos maiores às classes menores).\n",
    "\n",
    "A biblioteca XGBoost (https://xgboost.readthedocs.io) tem um parâmetro 'max_delta_step', que prode ser definido com um valor entre 1 e 10 para deixar o passo de atualização mais conservador. Há também um parâmetro 'scale_pos_weight' que define a razão entre amostras negativas e positivas (para classes binárias). Além do mais, 'eval_metric' deve ser definido com 'auc', em vez de usar o valor default igual a 'error' para classificação.\n",
    "\n",
    "O modelo KNN tem um parâmetro 'weights' que pode gerar distorção  em vizinhos mais próximos. Se as amostras da classe minoritária estiverem próximas, definir esse parâmetro com 'distance' poderá melhorar o desempenho."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83450d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.experimental import (\n",
    "    enable_iterative_imputer,\n",
    ")\n",
    "from sklearn import (\n",
    "    ensemble,\n",
    "    impute,\n",
    "    model_selection,    \n",
    "    preprocessing,\n",
    "    tree,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51963552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caminho em Pasta\n",
    "path = \"datasets/titanic/titanic3.xls\"\n",
    "df = pd.read_excel(path)\n",
    "orig_df = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f221739d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweak_titanic(df):\n",
    "    df = df.drop(\n",
    "        columns=[\n",
    "            \"name\",\n",
    "            \"ticket\",\n",
    "            \"home.dest\",\n",
    "            \"boat\",\n",
    "            \"body\",\n",
    "            \"cabin\",\n",
    "        ]\n",
    "    ).pipe(pd.get_dummies, drop_first=True)\n",
    "    return df\n",
    "def get_train_test_X_y(\n",
    "    df, y_col, size=0.3, std_cols=None\n",
    "):\n",
    "    y = df[y_col]\n",
    "    X = df.drop(columns=y_col)\n",
    "    X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
    "        X, y, test_size=size, random_state=42\n",
    "    )\n",
    "    cols = X.columns\n",
    "    num_cols = [\n",
    "        \"pclass\",\n",
    "        \"age\",\n",
    "        \"sibsp\",\n",
    "        \"parch\",\n",
    "        \"fare\",\n",
    "    ]\n",
    "    fi = impute.IterativeImputer()\n",
    "\n",
    "    fitted = fi.fit_transform(X_train[num_cols])\n",
    "    X_train = X_train.assign(**{c:fitted[:,i] for i, c in enumerate(num_cols)})\n",
    "    test_fit = fi.transform(X_test[num_cols])\n",
    "    X_test = X_test.assign(**{c:test_fit[:,i] for i, c in enumerate(num_cols)})\n",
    "    if std_cols:\n",
    "        std = preprocessing.StandardScaler()\n",
    "        fitted = std.fit_transform(X_train[std_cols])\n",
    "        X_train = X_train.assign(**{c:fitted[:,i] for i, c in enumerate(std_cols)})\n",
    "        test_fit = std.transform(X_test[std_cols])\n",
    "        X_test = X_test.assign(**{c:test_fit[:,i] for i, c in enumerate(std_cols)})\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "ti_df = tweak_titanic(df)\n",
    "std_cols = \"pclass,age,sibsp,fare\".split(\",\")\n",
    "X_train, X_test, y_train, y_test = get_train_test_X_y(\n",
    "    ti_df, \"survived\", std_cols=std_cols\n",
    ")\n",
    "\n",
    "X = pd.concat([X_train, X_test])\n",
    "y = pd.concat([y_train, y_test])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2030a02",
   "metadata": {},
   "source": [
    "### Upsampling da minoria\n",
    "\n",
    "Pode fazer um upsampling da classe minoritária de algumas maneiras. Segue uma implementação com o sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2c1a71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "mask = df.survived == 1\n",
    "surv_df = df[mask]\n",
    "death_df = df[~mask]\n",
    "df_upsample = resample(\n",
    "    surv_df, \n",
    "    replace=True,\n",
    "    n_samples=len(death_df),\n",
    "    random_state=42,\n",
    ")\n",
    "df = pd.concat([death_df, df_upsample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80b1fe64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    809\n",
       "1    809\n",
       "Name: survived, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.survived.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34adf51c",
   "metadata": {},
   "source": [
    "A biblioteca 'imbalanced-learn' também pode ser usada para amostrar aleatoriamente com substituição:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6ff0369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imblearn in /home/luciano/anaconda3/lib/python3.9/site-packages (0.0)\n",
      "Requirement already satisfied: imbalanced-learn in /home/luciano/anaconda3/lib/python3.9/site-packages (from imblearn) (0.10.1)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /home/luciano/anaconda3/lib/python3.9/site-packages (from imbalanced-learn->imblearn) (1.9.1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /home/luciano/anaconda3/lib/python3.9/site-packages (from imbalanced-learn->imblearn) (1.21.5)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/luciano/anaconda3/lib/python3.9/site-packages (from imbalanced-learn->imblearn) (2.2.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/luciano/anaconda3/lib/python3.9/site-packages (from imbalanced-learn->imblearn) (1.2.0)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in /home/luciano/anaconda3/lib/python3.9/site-packages (from imbalanced-learn->imblearn) (1.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ec6429e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    809\n",
       "1    809\n",
       "Name: survived, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imblearn.over_sampling import (\n",
    "    RandomOverSampler,\n",
    ")\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_ros, y_ros = ros.fit_resample(X, y)\n",
    "pd.Series(y_ros).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97c8a02",
   "metadata": {},
   "source": [
    "### Gerando dados de minorias\n",
    "\n",
    "A biblioteca imbalanced-learn também pode gerar novas amostras das classes minoritárias com os algoritmos para amostragens como 'SMOTE (Synthetic Minority Oversampling Technique)' e 'ADASYN (Adaptive Synthetic)'. O SMOTE funciona selecionando um de seus k vizinhos mais próximos, conectando uma linha a um deles e selecionando um ponto nessa linha. O ADASYN é semelhante ao SMOT, mas gera mais amostras a partir daquelas cujo aprendizado é mais díficil. As classes em imbalanced-learn se chamam 'over_sampling.SMOTE' e 'over_sampling.ADASYN'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92c5a1c",
   "metadata": {},
   "source": [
    "### Downsasmpling da minoria\n",
    "\n",
    "Outro método para balancear classes é fazer o downsampling das classes majoritárias. Segue um exemplo com o sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5cf375c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "mask = df.survived == 1\n",
    "surv_df = df[mask]\n",
    "death_df = df[~mask]\n",
    "df_downsample = resample(\n",
    "    death_df,\n",
    "    replace=False,\n",
    "    n_samples=len(surv_df),\n",
    "    random_state=42,\n",
    ")\n",
    "df = pd.concat([surv_df, df_downsample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80b635b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    809\n",
       "0    809\n",
       "Name: survived, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.survived.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82d13dc",
   "metadata": {},
   "source": [
    "###### DICA => Não use substituição quando fizer um downsampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43444fe6",
   "metadata": {},
   "source": [
    "A biblioteca 'imbalanced-learn' também implementa diversos algoritmos de downsampling:\n",
    "\n",
    "* ClusterCentroids => Essa classe utiliza k-means (k-médias) para sintetizar dados com os centróides.\n",
    "\n",
    "* RandomUnderSampler => Essa classe seleciona amostras aleatoriamente.\n",
    "\n",
    "* NearMiss => Essa classe faz um downsampling removendo amostras que estão próximas umas das outras.\n",
    "\n",
    "* TomekLink => Essa classe reduz as amostras removendo aquelas que estão mais próximas entre si.\n",
    "\n",
    "* EditedNearestNeighbours => Essa classe remove amostras que tenham vizinhos  que não estão na classe majoritária ou que estejam todos na mesma classe.\n",
    "\n",
    "* RepeatedNearestNeighbours => Essa classe chama 'EditedNearestNeighbours' repetidamente.\n",
    "\n",
    "* AllKNN => Essa classe é semelhante, mas aumenta o número de vizinhos mais próximos durante as iterações do downsampling.\n",
    "\n",
    "* CondensedNearestNeighbour => Essa classe escolhe uma amostra da classe para downsampling e, em seguida, itera pelas outras amostras da classe; se KNN não fizer uma classificação incorreta, essa amostra será adicionada.\n",
    "\n",
    "* OneSidedSelection => Essa classe remove amostras com ruído.\n",
    "\n",
    "* NeighbourhoodCleaningRule => Essa classe usa os resultados de 'EditedNearestNeighbour', aplicando aí o KNN.\n",
    "\n",
    "* InstanceHardnessThreshold => Essa classe faz o treinamento de um modelo e então remove as amostras com baixas probabilidades. \n",
    "\n",
    "Todas essas classes aceitam o método '.fit_resample'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2c2903",
   "metadata": {},
   "source": [
    "### Upsampling e depois downsampling\n",
    "\n",
    "A biblioteca 'imbalanced-learn' implementa 'SMOTEEN' e 'SMOTETOMEK', que fazem um umpsampling e depois aplicam downsampling para limpar os dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea46d04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
